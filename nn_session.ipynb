{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e94ef844-bbb6-42bb-95e4-817818ff19fa",
      "metadata": {
        "id": "e94ef844-bbb6-42bb-95e4-817818ff19fa"
      },
      "source": [
        "# Neural Networks Skills Session\n",
        "*Notebook for my skills session on neural networks, updated for 28/02/2025*\n",
        "\n",
        "The aims for this skills session are to:\n",
        "- Give you an idea of what a neural network actually is\n",
        "- Show you what the caveats are (they aren't actually magic)\n",
        "- Get you training a neural network to emulate simple stellar evolution data\n",
        "\n",
        "\n",
        "Get started with some imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b79411b-d1a0-41c9-aecf-606b06571982",
      "metadata": {
        "id": "7b79411b-d1a0-41c9-aecf-606b06571982"
      },
      "outputs": [],
      "source": [
        "!pip install cmcrameri # <- this is a package for colour-vision deficiency friendly colourmaps. I highly recommend giving them a go :)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from cmcrameri import cm\n",
        "\n",
        "# plt.style.use(\"Solarize_Light2\")\n",
        "# plt.rcParams.update({\"axes.edgecolor\": \"black\"})\n",
        "# plt.rcParams.update({\"text.color\": \"black\"})\n",
        "# plt.rcParams.update({\"axes.labelcolor\": \"black\"})\n",
        "# plt.rcParams.update({\"xtick.color\": \"black\"})\n",
        "# plt.rcParams.update({\"ytick.color\": \"black\"})\n",
        "# plt.rcParams.update({\"font.family\": \"monospace\"})\n",
        "\n",
        "\n",
        "plt.style.use(\"dark_background\")\n",
        "plt.rcParams.update({\"font.family\": \"monospace\"})\n",
        "colours = [colour[\"color\"] for colour in plt.rcParams[\"axes.prop_cycle\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539a5425-5a1f-4d50-b74a-8675e79b4601",
      "metadata": {
        "id": "539a5425-5a1f-4d50-b74a-8675e79b4601"
      },
      "source": [
        "# 1 Neural network basics\n",
        "Neural networks are a type of machine learning model which learn to reproduce patterns from the data they are trained on.\n",
        "\n",
        "They consist of a series of interconnected ***layers*** which are populated by one or more ***neurons***. Inputs are passed through a series of neurons before a prediction is made on the corresponding output:\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:805/0*Y4xTJQNCU3uvk0up.png)\n",
        "\n",
        "Seeing diagrams like this might be what has stopped you from looking any further into learning how to train your own neural networks, but I promise it really isn't as bad as it looks!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 What are neurons?\n",
        "Each node in the image above is a singe neuron in an example network. But what's going on inside each neuron?\n",
        "\n",
        "To get an output $y$ from some input $x$, we just apply a linear transformation of the form\n",
        "\\begin{equation}\n",
        "  y = f(wx+b),\n",
        "\\end{equation}\n",
        "where $w$ is a ***weight*** term, $b$ is a ***bias*** term. It's these weights and biases that the neural network optimises during training ($f(\\cdot)$ is the ***activation function***, we'll come back to this later!)\n",
        "\n",
        "A neural network consisting of a single layer, with a single neuron, and a linear activation function (i.e $f(a) = a$) is just optimising a linear fit by tuning the neuron weights and biases.\n",
        "\n",
        "Let's see how this looks by training a network to recreate a linear relationship $y = w*x + b$ with a randomly generated training set of $x$ values:"
      ],
      "metadata": {
        "id": "QIEcfR5aiOlp"
      },
      "id": "QIEcfR5aiOlp"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weight, bias):\n",
        "  ##### generate inputs #####\n",
        "  x = np.random.rand(1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y = weight*x + bias\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')\n",
        "\n",
        "  linear_model.fit(x,y,epochs=50,batch_size=10, verbose=0)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weight = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weight= 1 #<--- input value here (between -1 and 1 please!)\n",
        "bias = -1 #<--- input value here (between -1 and 1 please!)\n",
        "\n",
        "linear_fit(weight, bias)"
      ],
      "metadata": {
        "id": "TYje1SHCrypi"
      },
      "id": "TYje1SHCrypi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, your printed weights and biases from your trained single neuron network should roughly match the inputted values.\n",
        "\n",
        "In many cases, we'll have more than one input into our neural network ($\\textbf{x}$).\n",
        "\n",
        "Then our single neuron takes this form:\n",
        "\n",
        "\n",
        "![](https://www.nickmccullum.com/images/python-deep-learning/understanding-neurons-deep-learning/activation-function.png)\n",
        "\n",
        "\n",
        "And our linear transformation just becomes\n",
        "\\begin{equation}\n",
        "  y = f(\\textbf{w} \\cdot \\textbf{x} +b),\n",
        "\\end{equation}\n",
        "where $\\textbf{w}$ contains multiple weights.\n",
        "\n",
        "Let's see how our simple one neuron network copes with multiple inputs:\n"
      ],
      "metadata": {
        "id": "gWZ_V7C1MQKL"
      },
      "id": "gWZ_V7C1MQKL"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weights, bias):\n",
        "  ##### generate inputs and outputs #####\n",
        "  dataset_size=1000\n",
        "  inputs={}\n",
        "  output=np.full(dataset_size, bias)\n",
        "\n",
        "  for idx in range(len(weights)):\n",
        "    x = np.random.rand(dataset_size)\n",
        "    inputs[f'x{idx}'] = x\n",
        "    output += weights[idx]*x\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(shape=(len(weights),)))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')\n",
        "\n",
        "  linear_model.fit(np.column_stack([inputs[f'x{idx}'] for idx in range(len(weights))]),output,epochs=100,batch_size=10, verbose=0)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weights = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weights= [0.5,0.8,-0.4,-0.2] #<--- input values here (between -1 and 1 please!)\n",
        "bias = 0.3 #<--- input value here (between -1 and 1 please!)\n",
        "\n",
        "linear_fit(weights, bias)"
      ],
      "metadata": {
        "id": "yPkovihiCnZx"
      },
      "id": "yPkovihiCnZx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks good so far (hopefully the printed wights and bias are a good fit!), but what if we want to start building up our network with more neurons to fit more complex functions?\n",
        "\n",
        "Let's go back to our model from the start, linking a single input $x$ to a single output $y$, but this time pass $x$ through three neurons, $n_1, n_2,$ and $n_3$, instead of just one.\n",
        "\n",
        "Given we know now what each neuron is doing, it's easy to construct a function linking $x$ to $y$:\n",
        "\n",
        "\\begin{equation}\n",
        "y = f(w_3f(w_2f(w_1x+b_1)+b_2)+b_3).\n",
        "\\end{equation}\n",
        "\n",
        "Now we know what is happening within each neuron, let's move on to the activation function."
      ],
      "metadata": {
        "id": "xxFRTwARS-iq"
      },
      "id": "xxFRTwARS-iq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Activation functions\n",
        "Activation functions are used to introduce non-linearities into neural networks to allow us to fit more complex functions.\n",
        "\n",
        "Before, we just used a linear activation function of the form\n",
        "\\begin{equation}\n",
        "f(\\textbf{w}\\cdot\\textbf{x} + b) = \\alpha(\\textbf{w}\\cdot\\textbf{x} + b) + \\beta,\n",
        "\\end{equation}\n",
        "where $(\\alpha, \\beta) = (1,0)$.\n",
        "\n",
        "It turns out linear activation functions aren't a good choice if we're looking for non-linearity!\n",
        "\n",
        "If we take our equation for a series of neurons from before:\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\alpha w_3(\\alpha w_2(\\alpha w_1x + b_3 + \\beta) + b_ 2 + \\beta) + b_3 + \\beta,\n",
        "\\end{equation}\n",
        "\n",
        "Given that our neuron weights and biases, $w_n$ and $b_n$, and our linear activation function terms, $\\alpha$ and $\\beta$, are all constants, this equation for our fit collapses down to\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\alpha^3 w_3 w_2 w_1 x + \\gamma,\n",
        "\\end{equation}\n",
        "\n",
        "where $\\gamma$ is a constant. It doesn't matter how much we tune our $w_n$ terms here, we're stuck with a linear fit!\n",
        "\n",
        "Due to its computational efficiency, popular choice is the Rectified Linear Unit (ReLU) function:\n",
        "\\begin{equation}\n",
        "f(x) = \\textrm{max}(0,x),\n",
        "\\end{equation}\n",
        "Plotting this, we get:"
      ],
      "metadata": {
        "id": "gtywYgT1SCfF"
      },
      "id": "gtywYgT1SCfF"
    },
    {
      "cell_type": "code",
      "source": [
        "def rectified_linear_unit(x):\n",
        "  return max(0.0, x)\n",
        "\n",
        "x = np.linspace(-1,1,1000)\n",
        "y = [rectified_linear_unit(x_val) for x_val in x]\n",
        "plt.plot(x,y, linewidth=3, c=colours[0])\n",
        "plt.axvline(0, linestyle='--')\n",
        "plt.title('ReLU activation function')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pdtsK_llg7tM"
      },
      "id": "pdtsK_llg7tM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be thinking that this still seems very close to a linear function.\n",
        "\n",
        "However, ReLU doesn't stick to the condition of a linear function that requires\n",
        "\\begin{equation}\n",
        "f(x) + f(y) = f(x+y),\n",
        "\\end{equation}\n",
        "because for ReLU $f(-1) + f(1) \\neq f(0)$.\n",
        "\n",
        "This means we avoid the function for our series of neurons collapsing down to just a linear fit, and we have non-linearity!\n",
        "\n",
        "Now, by increasing the number of layers in our network, and the number of neurons per layer, we're able to effectively reproduce complex functions present in a dataset used for training.\n",
        "\n",
        "For the rest of this notebook we'll stick to just using ReLU, but I'll show you a couple of other activation functions here:"
      ],
      "metadata": {
        "id": "t9wbCrPQjUg7"
      },
      "id": "t9wbCrPQjUg7"
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-4,4,1000)\n",
        "\n",
        "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows=2,ncols=2, figsize=(10,10))\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+(np.e**-x))\n",
        "\n",
        "y = [sigmoid(x_val) for x_val in x]\n",
        "\n",
        "ax1.plot(x,y,c=colours[0], linewidth=3)\n",
        "ax1.axvline(0, linestyle='--')\n",
        "ax1.axhline(0, linestyle='--')\n",
        "ax1.set_title('sigmoid')\n",
        "\n",
        "def tanh(x):\n",
        "  return ((np.e**x)-(np.e**-x))/((np.e**x)+(np.e**-x))\n",
        "\n",
        "y = [tanh(x_val) for x_val in x]\n",
        "\n",
        "ax2.plot(x,y,c=colours[1], linewidth=3)\n",
        "ax2.axvline(0, linestyle='--')\n",
        "ax2.axhline(0, linestyle='--')\n",
        "ax2.set_title('tanh')\n",
        "\n",
        "def elu(x):\n",
        "  if x >= 0:\n",
        "    y = x\n",
        "  else:\n",
        "    y = (np.e**x)-1\n",
        "  return y\n",
        "\n",
        "y = [elu(x_val) for x_val in x]\n",
        "\n",
        "ax3.plot(x,y,c=colours[2], linewidth=3)\n",
        "ax3.axvline(0, linestyle='--')\n",
        "ax3.axhline(0, linestyle='--')\n",
        "ax3.set_title('ELU')\n",
        "\n",
        "def swish(x):\n",
        "  return x*sigmoid(x)\n",
        "\n",
        "y = [swish(x_val) for x_val in x]\n",
        "\n",
        "ax4.plot(x,y,c=colours[3], linewidth=3)\n",
        "ax4.axvline(0, linestyle='--')\n",
        "ax4.axhline(0, linestyle='--')\n",
        "ax4.set_title('swish')\n",
        "\n",
        "\n",
        "plt.suptitle('a few different activation functions', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMjdy8Evm3VO"
      },
      "id": "sMjdy8Evm3VO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Loss functions\n",
        "The last part of a basic neural network we need to learn about is the ***loss function*** - this is how our neural network determines how well it is fitting the data.\n",
        "\n",
        "For a every point in our dataset of $N$ points, each with an $x$ and $y$ value, we show our network the input $x$ and it makes a prediction $\\hat{y}$.\n",
        "\n",
        "A common choice is to take the Mean Squared Error (MSE), averaged over all $N$ points:\n",
        "\\begin{equation}\n",
        "\\textrm{MSE} = \\frac{1}{N} \\sum_{y=i}^{N} (y_i-\\hat{y}_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "During training, our network tunes the neuron weights and biases in a way that decreases the MSE over the dataset.\n",
        "\n",
        "I turned off the in-training network printout from the cells for our single neuron, but lets run them again with the printout enabled (setting verbose=1 in model.fit) to see what we get:"
      ],
      "metadata": {
        "id": "GXtJj-ItoG-U"
      },
      "id": "GXtJj-ItoG-U"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weight, bias):\n",
        "  ##### generate inputs #####\n",
        "  x = np.random.rand(1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y = weight*x + bias\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')# <- here's where we define our loss function!\n",
        "\n",
        "  linear_model.fit(x,y,epochs=50,batch_size=10, verbose=1)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weight = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weight= 0.5 #<--- input value here (between -1 and 1 please!)\n",
        "bias = 0.8 #<--- input value here (between -1 and 1 please!)\n",
        "\n",
        "linear_fit(weight, bias)"
      ],
      "metadata": {
        "id": "zrHf-bNvqa9e"
      },
      "id": "zrHf-bNvqa9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, you should get a series of readouts that look something like this:\n",
        "\n",
        "> Epoch 1/50\\\n",
        "100/100 [==============================] - 1s 2ms/step - loss: 2.2699\\\n",
        "Epoch 2/50\\\n",
        "100/100 [==============================] - 0s 2ms/step - loss: 1.8381\\\n",
        "Epoch 3/50\\\n",
        "100/100 [==============================] - 0s 1ms/step - loss: 1.4577,\n",
        "\n",
        "with the 'loss' value decreasing each epoch (each pass through the dataset). This is your neural network learning!"
      ],
      "metadata": {
        "id": "b-ZwWU5AqlKn"
      },
      "id": "b-ZwWU5AqlKn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Neural Network Caveats\n",
        "Now we know what's going on inside a neural network, let's go over the balancing act between under and overfitting, and the dangers of extrapolation!\n"
      ],
      "metadata": {
        "id": "Cb8n_Y7RnJaC"
      },
      "id": "Cb8n_Y7RnJaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Underfitting\n",
        "Typically, underfitting is an indication that we aren't using a neural network architecture that is complex enough to fit our target function properly.\n",
        "\n",
        "Let's say we want to train a neural network to reproduce the function\n",
        "\\begin{equation}\n",
        "y = \\textrm{sin}(x)\n",
        "\\end{equation}\n",
        "And we take some samples from this function to train our simple one neuron network:"
      ],
      "metadata": {
        "id": "ceSDX2e5z150"
      },
      "id": "ceSDX2e5z150"
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_fit(layers, neurons_per_layer):\n",
        "  ##### generate inputs #####\n",
        "  x_train = np.random.uniform(low=-3*np.pi, high=3*np.pi, size=1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y_train = np.sin(x_train)\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  sin_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  sin_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    sin_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"relu\",))\n",
        "\n",
        "  sin_model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "  sin_model.compile(loss='mse')\n",
        "\n",
        "  sin_model.fit(x_train,y_train,epochs=20,batch_size=10, verbose=1)\n",
        "\n",
        "  return sin_model\n",
        "\n",
        "layers = 2 #<--- input value here\n",
        "neurons_per_layer = 1 #<--- input value here\n",
        "\n",
        "sin_model = sin_fit(layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "YW9O87pE2tt0"
      },
      "id": "YW9O87pE2tt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, your network should end up with an MSE loss score of $\\sim0.5$\n",
        "\n",
        "Now our network is trained, let's see how well it reproduces the function we used to generate our samples by testing it on some test data, again drawn from our function\n",
        "\\begin{equation}\n",
        "y_{test} = \\textrm{sin}(x_{test})\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "IohZjY4w6KFb"
      },
      "id": "IohZjY4w6KFb"
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-3*np.pi, 3*np.pi,1000)\n",
        "y_test = np.sin(x_test)\n",
        "\n",
        "y_pred = np.array(sin_model(x_test)).flatten()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.xlim(-3*np.pi, 3*np.pi)\n",
        "plt.legend()\n",
        "plt.title('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m2lG2BTu6fUL"
      },
      "id": "m2lG2BTu6fUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not good!\n",
        "\n",
        "This network doesn't have enough neurons to solve this problem!\n",
        "\n",
        "One layer with one neuron just isn't complex enough to fit anything other than a straight line (even with the non-linearity from our ReLU activation function), and has just learned that predicting $\\hat{y} = 0$ for all $x$ is the best it can do to minimise the loss score.\n",
        "\n",
        "Let's try again with a more complex network, what about 2 layers with 100 neurons?\n"
      ],
      "metadata": {
        "id": "omNdOWKf8jnC"
      },
      "id": "omNdOWKf8jnC"
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_fit(layers, neurons_per_layer):\n",
        "  ##### generate inputs #####\n",
        "  x_train = np.random.uniform(low=-3*np.pi, high=3*np.pi, size=1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y_train = np.sin(x_train)\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  sin_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  sin_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    sin_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"relu\",))\n",
        "\n",
        "  sin_model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "  sin_model.compile(loss='mse')\n",
        "\n",
        "  sin_model.fit(x_train,y_train,epochs=100,batch_size=10, verbose=1)\n",
        "\n",
        "  return sin_model\n",
        "\n",
        "layers = 2 #<--- input value here\n",
        "neurons_per_layer = 100 #<--- input value here\n",
        "\n",
        "sin_model = sin_fit(layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "3K2IGBK6ED5P"
      },
      "id": "3K2IGBK6ED5P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your loss scores should be much better now! Again, let's see how we do when predicting test points:"
      ],
      "metadata": {
        "id": "tJnifDpUGOmq"
      },
      "id": "tJnifDpUGOmq"
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-3*np.pi, 3*np.pi,1000)\n",
        "y_test = np.sin(x_test)\n",
        "\n",
        "y_pred = np.array(sin_model(x_test)).flatten()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.xlim(-3*np.pi, 3*np.pi)\n",
        "plt.legend()\n",
        "plt.title('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IqrEBFWqE37N"
      },
      "id": "IqrEBFWqE37N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We solved our underfitting problem by just making our neural network architecture more complex.\n",
        "\n",
        "We can also fix this problem by training our neural networks for longer.\n",
        "\n",
        "So why don't we just use extremely dense networks trained for many training epochs for every problem?"
      ],
      "metadata": {
        "id": "_vBNf7HhGHUE"
      },
      "id": "_vBNf7HhGHUE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Overfitting\n",
        "Overfitting is when a neural network starts to fit noise that is present in the training set instead of the general underlying function that we want it to learn.\n",
        "\n",
        "Let's say we have some datapoints that roughly follow the trend $y = x$ but with some gaussian noise applied:"
      ],
      "metadata": {
        "id": "oLvUi3pRD0wG"
      },
      "id": "oLvUi3pRD0wG"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "x_train = np.linspace(-1,1,10)\n",
        "\n",
        "y_train = x_train + np.random.normal(loc=0, scale=0.2, size=10)\n",
        "\n",
        "plt.scatter(x_train,y_train, c=colours[7], label='training set points')\n",
        "plt.plot([-100,100], [-100,100], c='w', label = 'y=x', linestyle = '--', alpha=0.5)\n",
        "plt.title('Training set')\n",
        "plt.xlim(-1.2, 1.2)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bz5wCk0JHfvq"
      },
      "id": "bz5wCk0JHfvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And following our success on the $y=\\textrm{sin}(x)$ data we decide to use a network with 8 layers, 64 neurons per layer, the ReLU activation function, and trained for many epochs to avoid underfitting:"
      ],
      "metadata": {
        "id": "zll9MtJeH3JR"
      },
      "id": "zll9MtJeH3JR"
    },
    {
      "cell_type": "code",
      "source": [
        "def x_fit(x_train, y_train, layers, neurons_per_layer):\n",
        "#### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  x_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  x_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    x_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"relu\"))\n",
        "\n",
        "  x_model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.03)\n",
        "  x_model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "  x_model.fit(x_train,y_train,epochs=300,batch_size=10, verbose=1,shuffle=True)\n",
        "\n",
        "  return x_model\n",
        "\n",
        "layers = 8 #<--- input value here\n",
        "neurons_per_layer = 64 #<--- input value here\n",
        "\n",
        "x_model = x_fit(x_train, y_train, layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "Thd_20gaH0t2"
      },
      "id": "Thd_20gaH0t2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, these MSE loss scores (should) look amazing!\n",
        "\n",
        "Let's test to see how our network has done by generating some test data following the same form, and see how the network's predictions look:"
      ],
      "metadata": {
        "id": "V3JeA_EHHlD8"
      },
      "id": "V3JeA_EHHlD8"
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-1,1,100)\n",
        "\n",
        "y_test = x_test + np.random.normal(loc=0, scale=0.2, size=100)\n",
        "\n",
        "y_pred = np.array(x_model(x_test)).flatten()\n",
        "\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.xlim(-1.2, 1.2)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0pk5Zf26IoE9"
      },
      "id": "0pk5Zf26IoE9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That certainly doesn't look like $y=x$...\n",
        "\n",
        "In fact, that form in the prediction line might look familiar - let's try plotting this over the training points we used to train the network to begin with:"
      ],
      "metadata": {
        "id": "3U6uPtHJIhx1"
      },
      "id": "3U6uPtHJIhx1"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.scatter(x_train,y_train, c=colours[7], label='training set')\n",
        "plt.xlim(-1.2, 1.2)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "moXq8FmDJCY2"
      },
      "id": "moXq8FmDJCY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network is too smart for its own good!\n",
        "\n",
        "The loss function is all the neural network has to learn to inform its weight and bias tuning.\n",
        "\n",
        "It was smart enough to learn a function far more complex than the $y=x$ we wanted it to learn, fit it to the datapoints in the training set including the noise, and achieve a loss score close to 0.\n",
        "\n",
        "Let's see if we can rectify this by reducing our network complexity. Let's try using just 2 layers each with 4 neurons, and a linear activation function:"
      ],
      "metadata": {
        "id": "PjBo5xNsHRgC"
      },
      "id": "PjBo5xNsHRgC"
    },
    {
      "cell_type": "code",
      "source": [
        "def x_fit(x_train, y_train, layers, neurons_per_layer):\n",
        "#### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  x_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  x_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    x_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"linear\"))\n",
        "\n",
        "  x_model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.03)\n",
        "  x_model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "  x_model.fit(x_train,y_train,epochs=100,batch_size=10, verbose=1,shuffle=True)\n",
        "\n",
        "  return x_model\n",
        "\n",
        "layers =  2#<--- input value here\n",
        "neurons_per_layer = 4 #<--- input value here\n",
        "\n",
        "x_model = x_fit(x_train, y_train, layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "hvtxDnWPNEqs"
      },
      "id": "hvtxDnWPNEqs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-1,1,100)\n",
        "\n",
        "y_test = x_test + np.random.normal(loc=0, scale=0.2, size=100)\n",
        "\n",
        "y_pred = np.array(x_model(x_test)).flatten()\n",
        "\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.xlim(-1.2, 1.2)\n",
        "plt.ylim(-1.2, 1.2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RlTI1x5zNWac"
      },
      "id": "RlTI1x5zNWac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smarter isn't always better!\n",
        "\n",
        "It turns out that a simpler network was much better at finding a general function for this dataset that doesn't fit to the noise than our complex one.\n",
        "\n",
        "Along with reducing network complexity, there are a couple of ways to detect and prevent overfitting while training a network:\n",
        "\n",
        "- We can use a 'validation' set during training alongside our training set - points in our validation set aren't shown to the network while it updates weights and biases. By calculating loss over this validation set (validation loss) as well as our training set loss, we can see whether the network has just learned to fit the training set - if training loss steadily decreases while validation loss plateaus or increases, we're overfitting.\n",
        "\n",
        "- Another solution is to just increase the number of points in our training set - this makes it much harder for our network to fit to the noise."
      ],
      "metadata": {
        "id": "KClUF7HVN4C1"
      },
      "id": "KClUF7HVN4C1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Extrapolation\n",
        "Neural networks are great at mimicking interpolation, generating new points that roughly match the form of the training set given inputs that lie ***within*** the parameter space they were trained on.\n",
        "\n",
        "However, if we ask them to extrapolate by giving them unreasonable inputs from outside the ranges of the training set, we get some funky results!\n",
        "\n",
        "Lets retrain a network that approximates $y=\\textrm{sin}(x)$ within the ranges $-3\\pi < x < 3\\pi$:"
      ],
      "metadata": {
        "id": "GiAjxS-eSoTW"
      },
      "id": "GiAjxS-eSoTW"
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_fit(layers, neurons_per_layer):\n",
        "  ##### generate inputs #####\n",
        "  x_train = np.random.uniform(low=-3*np.pi, high=3*np.pi, size=1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y_train = np.sin(x_train)\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  sin_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  sin_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    sin_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"relu\",))\n",
        "\n",
        "  sin_model.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "  sin_model.compile(loss='mse')\n",
        "\n",
        "  sin_model.fit(x_train,y_train,epochs=100,batch_size=10, verbose=1)\n",
        "\n",
        "  return sin_model\n",
        "\n",
        "layers = 2 #<--- input value here\n",
        "neurons_per_layer = 100 #<--- input value here\n",
        "\n",
        "sin_model = sin_fit(layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "TrNlWHeU1MmW"
      },
      "id": "TrNlWHeU1MmW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loss looks good, let's try plotting some test point predictions to see how we've done:"
      ],
      "metadata": {
        "id": "0v-eQulvUQ9i"
      },
      "id": "0v-eQulvUQ9i"
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-3*np.pi, 3*np.pi,1000)\n",
        "y_test = np.sin(x_test)\n",
        "\n",
        "y_pred = np.array(sin_model(x_test)).flatten()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.xlim(-3*np.pi, 3*np.pi)\n",
        "plt.legend()\n",
        "plt.title('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TeGpKAXeUdJ_"
      },
      "id": "TeGpKAXeUdJ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Close to the middle of the trained $x$ range of $-3\\pi < x < 3\\pi$ things look pretty good.\n",
        "\n",
        "However, you can probably see that towards the edges of this range our network's predictions begin to stray away from $y=sin(x)$.\n",
        "\n",
        "This is an important caveat of neural networks: they accurately approximate a function within the parameter space of the training data, they don't learn the function itself.\n",
        "\n",
        "Our network has updated its weights and biases in a way that closely mimics $y=\\textrm{sin}(x)$ in our $x$ range, but it has not learned to approximate $y=\\textrm{sin}(x)$ for all $x$.\n",
        "\n",
        "Let's see what happens if we ask it to predict for a larger $x$ range of $-10\\pi < x < 10\\pi$:"
      ],
      "metadata": {
        "id": "gsNNcXZtUe_2"
      },
      "id": "gsNNcXZtUe_2"
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.linspace(-10*np.pi, 10*np.pi,1000)\n",
        "y_test = np.sin(x_test)\n",
        "\n",
        "y_pred = np.array(sin_model(x_test)).flatten()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.scatter(x_test,y_test, c=colours[0], label='test set')\n",
        "plt.scatter(x_test,y_pred, c=colours[3], label='predictions on test set')\n",
        "plt.axvline(-3*np.pi, c='w', linestyle='--', alpha=0.5, label='training set range')\n",
        "plt.axvline(3*np.pi, c='w', linestyle='--', alpha=0.5)\n",
        "plt.xlim(-10*np.pi, 10*np.pi)\n",
        "plt.legend()\n",
        "plt.title('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1hzQa_ZWI34"
      },
      "id": "S1hzQa_ZWI34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Being aware of dodgy extrapolation is important - neural networks don't know what a 'reasonable' input value is. They will confidently make a prediction regardless!\n",
        "\n",
        "While this is something to be aware of, it can be fun - in the next section we can ask a neural network to emulate solar evolution forwards to three times the age of the universe, or backwards to negative age!"
      ],
      "metadata": {
        "id": "9jl9s1JtWRLO"
      },
      "id": "9jl9s1JtWRLO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Emulating Stellar Evolution\n",
        "Now you know about neural networks, let's get some astronomy involved by training a neural network to emulate a solar-like evolutionary track by predicting luminosity and effective temperature for a given age.\n",
        "\n",
        "First let's take a look at the data:"
      ],
      "metadata": {
        "id": "HMgptE7qX1dt"
      },
      "id": "HMgptE7qX1dt"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "solar_track_df = pd.read_csv('solar_track.csv')\n",
        "\n",
        "del uploaded\n",
        "\n",
        "age = solar_track_df['star_age'].values\n",
        "luminosity = solar_track_df['luminosity'].values\n",
        "effective_T = solar_track_df['effective_T'].values\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(effective_T,luminosity, c=age, label='training data',cmap=cm.imola)\n",
        "plt.gca().invert_xaxis()\n",
        "plt.title('Solar-like track from grid')\n",
        "plt.xlabel('Effective Temperature, K')\n",
        "plt.ylabel('Luminosity, $L_{\\odot}$')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Age, Gyr')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UytpQP9Jbc2O"
      },
      "id": "UytpQP9Jbc2O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try training a neural network to predict Luminosity and Temperature given age!\n",
        "\n",
        "We'll start off with a fairly simple network - 8 layers each with 16 units and a ReLU activation function.\n",
        "\n",
        "It's worth noting here that I scale the data at the start to be between 0 and 1 - this stops features with a large dynamic range (temperature, in this case) from dominating the loss optimisation.\n",
        "\n",
        "Also, when defining the layers in the network, we add an extra unit to the output layer to account for our two outputs (Effective Temperature and Luminosity)"
      ],
      "metadata": {
        "id": "XAwmh64JUwBc"
      },
      "id": "XAwmh64JUwBc"
    },
    {
      "cell_type": "code",
      "source": [
        "#### Scale data\n",
        "train_age = (age-age.min())/(age.max()-age.min())\n",
        "train_luminosity = (luminosity-luminosity.min())/(luminosity.max() -luminosity.min())\n",
        "train_effective_T = (effective_T-effective_T.min())/(effective_T.max()-effective_T.min())\n",
        "\n",
        "def solar_fit(x_train, y_train, layers, neurons_per_layer):\n",
        "#### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  solar_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  solar_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    solar_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=\"relu\"))\n",
        "\n",
        "  solar_model.add(tf.keras.layers.Dense(2, activation=\"linear\"))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "  solar_model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "  solar_model.fit(x_train,y_train,epochs=50,batch_size=1, verbose=1,shuffle=True)\n",
        "\n",
        "  return solar_model\n",
        "\n",
        "layers = 8\n",
        "neurons_per_layer = 16\n",
        "\n",
        "solar_model = solar_fit(train_age, np.column_stack((train_effective_T, train_luminosity)), layers, neurons_per_layer)"
      ],
      "metadata": {
        "id": "12APtZe428mA"
      },
      "id": "12APtZe428mA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now lets try predicting L and T over a dense and regularly sampled age range:"
      ],
      "metadata": {
        "id": "boMd1A4xVKvc"
      },
      "id": "boMd1A4xVKvc"
    },
    {
      "cell_type": "code",
      "source": [
        "test_age = (np.linspace(age.min(),age.max(),10000)-age.min())/(age.max()-age.min())\n",
        "\n",
        "preds = solar_model(test_age)\n",
        "\n",
        "pred_effective_T = (preds[:,0]*(effective_T.max()-effective_T.min()))+effective_T.min()\n",
        "pred_luminosity = (preds[:,1]*(luminosity.max()-luminosity.min()))+luminosity.min()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(effective_T,luminosity, c=age, label='training data', cmap=cm.imola)\n",
        "plt.plot(pred_effective_T,pred_luminosity, color=colours[3], label='predictions', linewidth=4)\n",
        "\n",
        "plt.gca().invert_xaxis()\n",
        "plt.title('Solar-like track from grid')\n",
        "plt.xlabel('Effective Temperature, K')\n",
        "plt.ylabel('Luminosity, $L_{\\odot}$')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Age, Gyr')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tXakviI6307o"
      },
      "id": "tXakviI6307o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll probably find that your predictions aren't great, particularly at high ages... We'll try to fix this in a second!\n",
        "\n",
        "But first, let's try some weird extrapolation. What asking what the sun will look like at 40Gyrs old? (Probably quite cold!)"
      ],
      "metadata": {
        "id": "eyxjd7oed-uv"
      },
      "id": "eyxjd7oed-uv"
    },
    {
      "cell_type": "code",
      "source": [
        "test_age = (np.linspace(age.min(),40,10000)-age.min())/(age.max()-age.min())\n",
        "\n",
        "preds = solar_model(test_age)\n",
        "\n",
        "pred_effective_T = (preds[:,0]*(effective_T.max()-effective_T.min()))+effective_T.min()\n",
        "pred_luminosity = (preds[:,1]*(luminosity.max()-luminosity.min()))+luminosity.min()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(effective_T,luminosity, c=age, label='training data', cmap=cm.imola)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Age, Gyr')\n",
        "plt.plot(pred_effective_T,pred_luminosity, color=colours[3], linewidth=3, label='predictions')\n",
        "plt.scatter(pred_effective_T[-1], pred_luminosity[-1], color=colours[3], label='40Gyr')\n",
        "\n",
        "plt.gca().invert_xaxis()\n",
        "plt.title('Solar-like track from grid')\n",
        "plt.xlabel('Effective Temperature, K')\n",
        "plt.ylabel('Luminosity, $L_{\\odot}$')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FzPcKQNpePR5"
      },
      "id": "FzPcKQNpePR5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or even better - what did the sun look like when it was -40Gyrs old?"
      ],
      "metadata": {
        "id": "_HZ4pCroeSyr"
      },
      "id": "_HZ4pCroeSyr"
    },
    {
      "cell_type": "code",
      "source": [
        "test_age = (np.linspace(-40,age.max(),10000)-age.min())/(age.max()-age.min())\n",
        "\n",
        "preds = solar_model(test_age)\n",
        "\n",
        "pred_effective_T = (preds[:,0]*(effective_T.max()-effective_T.min()))+effective_T.min()\n",
        "pred_luminosity = (preds[:,1]*(luminosity.max()-luminosity.min()))+luminosity.min()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(effective_T,luminosity, c=age, label='training data', cmap=cm.imola)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Age, Gyr')\n",
        "plt.plot(pred_effective_T,pred_luminosity, color=colours[3], linewidth=3, label='predictions')\n",
        "plt.scatter(pred_effective_T[0], pred_luminosity[0], color=colours[3], label='-40Gyr')\n",
        "\n",
        "plt.gca().invert_xaxis()\n",
        "plt.title('Solar-like track from grid')\n",
        "plt.xlabel('Effective Temperature, K')\n",
        "plt.ylabel('Luminosity, $L_{\\odot}$')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XEJZ7bZHfD7Q"
      },
      "id": "XEJZ7bZHfD7Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weird! This is another example of neural networks being able to predict on any input, and why we need to be careful about only querying the network within parameter ranges it has been trained on.\n",
        "\n",
        "For the rest of the session, feel free to mess around with the code below to try to optimise your own network by tweaking the different hyperparameters.\n",
        "\n",
        "Here's a short description of roughly what each of these parameters controls:\n",
        "\n",
        "*   Layers\n",
        "  * The \"depth\" of the neural network\n",
        "  * Controls the maximum complexity of the features the neural network can extract, with more depth increasing the maximum complexity\n",
        "  * Beware! Too many layers for the task can lead to overfitting your data\n",
        "*   Neurons per layer\n",
        "  * The \"width\" of the neural network\n",
        "  * Also controls the maximum complexity of the feature extraction\n",
        "  * Also a major factor in the likelihood of overfitting your data\n",
        "* Activation function\n",
        "  * Applied to the weight and bias terms in each neuron\n",
        "  * Adds non-linearity to the model, allowing to fit more complex patterns\n",
        "  * More complex activation functions can slow down training and prediction times especially with very wide and deep models, because they're called at *every* neuron\n",
        "* Learning rate\n",
        "  * Controls how fast the neural network navigates the loss landscape\n",
        "  * A high learning rate can helps us find a minima quickly, and jump out of local loss minimum to find the global minimum, but doesn't allow dropping into the very bottom of the minimum\n",
        "  * A lower learning rate means we can drop right into the bottom of a loss mimum, but also increases the risk of dropping into a local minimum and not escaping again\n",
        "* Epochs\n",
        "  * How many times to pass through the training set\n",
        "  * More epochs give our network more time to learn, but also increases the risk of overfitting!\n",
        "  * Too few epochs and our network might not get a chance to learn anything\n",
        "* Batch size\n",
        "  * Controls the number of training points that are passed to the network in chunks during training until we have passed through the entire training set and move onto the next epoch\n",
        "  * Higher batch size trains much faster (usually by leveraging GPU vectorisation) and means our network learns to generalise over large populations of training points\n",
        "  * Lower batch size will train much lower and our network will be less capable of generalisation, but will potentially be better able to cope with outliers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oXxDpGWgtfIv"
      },
      "id": "oXxDpGWgtfIv"
    },
    {
      "cell_type": "code",
      "source": [
        "################################ v your values here v ##########################\n",
        "\n",
        "layers = 8\n",
        "\n",
        "neurons_per_layer = 16\n",
        "\n",
        "activation_function = 'relu' #'linear','swish','sigmoid','elu','tanh'\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "epochs=50\n",
        "\n",
        "batch_size=1\n",
        "\n",
        "\n",
        "######################################training##################################\n",
        "#### Scale data\n",
        "train_age = (age-age.min())/(age.max()-age.min())\n",
        "train_luminosity = (luminosity-luminosity.min())/(luminosity.max() -luminosity.min())\n",
        "train_effective_T = (effective_T-effective_T.min())/(effective_T.max()-effective_T.min())\n",
        "\n",
        "def solar_fit(x_train, y_train,\n",
        "              layers, neurons_per_layer,\n",
        "              activation_function='relu',\n",
        "              learning_rate=0.0005,\n",
        "              epochs=50,\n",
        "              batch_size=1\n",
        "              ):\n",
        "\n",
        "  #### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  solar_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  solar_model.add(tf.keras.Input(shape=(1,)))#input layer\n",
        "\n",
        "  for layer in range(layers):\n",
        "    solar_model.add(tf.keras.layers.Dense(units=int(neurons_per_layer), activation=activation_function))\n",
        "\n",
        "  solar_model.add(tf.keras.layers.Dense(2, activation=\"linear\"))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  solar_model.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "  solar_model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size, verbose=1,shuffle=True)\n",
        "\n",
        "  return solar_model\n",
        "\n",
        "solar_model = solar_fit(train_age, np.column_stack((train_effective_T, train_luminosity)),\n",
        "                        layers, neurons_per_layer,\n",
        "                        activation_function=activation_function,\n",
        "                        learning_rate=learning_rate,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size\n",
        "                        )\n",
        "\n",
        "##################################plotting######################################\n",
        "\n",
        "test_age = (np.linspace(age.min(),age.max(),10000)-age.min())/(age.max()-age.min())\n",
        "\n",
        "preds = solar_model(test_age)\n",
        "\n",
        "pred_effective_T = (preds[:,0]*(effective_T.max()-effective_T.min()))+effective_T.min()\n",
        "pred_luminosity = (preds[:,1]*(luminosity.max()-luminosity.min()))+luminosity.min()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(effective_T,luminosity, c=age, label='training data', cmap=cm.imola)\n",
        "plt.plot(pred_effective_T,pred_luminosity, color=colours[3], label='predictions', linewidth=4)\n",
        "\n",
        "plt.gca().invert_xaxis()\n",
        "plt.title('Solar-like track from grid')\n",
        "plt.xlabel('Effective Temperature, K')\n",
        "plt.ylabel('Luminosity, $L_{\\odot}$')\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Age, Gyr')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5OJzbgBN41se"
      },
      "id": "5OJzbgBN41se",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HO4bLjRvhMU"
      },
      "id": "8HO4bLjRvhMU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}