{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e94ef844-bbb6-42bb-95e4-817818ff19fa",
      "metadata": {
        "id": "e94ef844-bbb6-42bb-95e4-817818ff19fa"
      },
      "source": [
        "# Neural Networks Skills Session\n",
        "*Notebook for my skills session on neural networks on 2024-03-01*\n",
        "\n",
        "The aims for this skills session are to:\n",
        "- Give you an idea of what a neural network actually is\n",
        "- Show you what the caveats are (they aren't actually magic)\n",
        "- Get you training a neural network to emulate simple stellar evolution data\n",
        "\n",
        "\n",
        "Get started with some imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7b79411b-d1a0-41c9-aecf-606b06571982",
      "metadata": {
        "id": "7b79411b-d1a0-41c9-aecf-606b06571982"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539a5425-5a1f-4d50-b74a-8675e79b4601",
      "metadata": {
        "id": "539a5425-5a1f-4d50-b74a-8675e79b4601"
      },
      "source": [
        "# 1 Neural network basics\n",
        "Neural networks are a type of machine learning model which learn to reproduce patterns from the data they are trained on.\n",
        "\n",
        "They consist of a series of interconnected ***layers*** which are populated by one or more ***neurons***. Inputs are passed through :\n",
        "\n",
        "***IMAGE OF HORRIBLE FULLY CONNECTED LAYERS HERE***\n",
        "\n",
        "Seeing diagrams like this might be what has stopped you from looking any further into learning how to train your own neural networks, but I promise it really isn't as bad as it looks!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 What are neurons?\n",
        "Each node in the image above is a singe neuron in an example network.\n",
        "\n",
        "Let's take a look at what a neuron looks like for a single-input network:\n",
        "\n",
        "***NEURON IMAGE HERE, SINGLE INPUT***\n",
        "\n",
        "To get an output $y$ from some input $x$, we just apply a linear transformation of the form\n",
        "\\begin{equation}\n",
        "  y = f(wx+b),\n",
        "\\end{equation}\n",
        "where $w$ is a ***weight*** term, $b$ is a ***bias*** term. ($f(\\cdot)$ is the ***activation function***, we'll come back to this later!)\n",
        "\n",
        "A neural network consisting of a single layer, with a single neuron, and a linear activation function (i.e $f(a) = a$) is just optimising a linear fit by tuning the neuron weights and biases.\n",
        "\n",
        "Let's see how this looks:"
      ],
      "metadata": {
        "id": "QIEcfR5aiOlp"
      },
      "id": "QIEcfR5aiOlp"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weight, bias):\n",
        "  ##### generate inputs #####\n",
        "  x = np.random.rand(1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y = weight*x + bias\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(1,))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')\n",
        "\n",
        "  linear_model.fit(x,y,epochs=50,batch_size=10, verbose=0)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weight = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weight= 0.5 #<--- input value here\n",
        "bias = 0.8 #<--- input value here\n",
        "\n",
        "linear_fit(weight, bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYje1SHCrypi",
        "outputId": "26b4a360-3937-4b78-ac0b-2f62d3946628"
      },
      "id": "TYje1SHCrypi",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear model weight = [[0.49981344]]\n",
            "linear model bias = [0.79993695]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases, we'll have more than one input into our networks ($\\textbf{x}$).\n",
        "\n",
        "Then our single neuron takes this form:\n",
        "\n",
        "***NEURON IMAGE HERE, MANY INPUTS***\n",
        "\n",
        "And our linear transformation just becomes\n",
        "\\begin{equation}\n",
        "  y = f(\\textbf{w} \\cdot \\textbf{x} +b),\n",
        "\\end{equation}\n",
        "where $\\textbf{w}$ contains multiple weights.\n",
        "\n",
        "Let's see how our simple one neuron network copes with multiple inputs:\n"
      ],
      "metadata": {
        "id": "gWZ_V7C1MQKL"
      },
      "id": "gWZ_V7C1MQKL"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weights, bias):\n",
        "  ##### generate inputs and outputs #####\n",
        "  dataset_size=1000\n",
        "  inputs={}\n",
        "  output=np.full(dataset_size, bias)\n",
        "\n",
        "  for idx in range(len(weights)):\n",
        "    x = np.random.rand(dataset_size)\n",
        "    inputs[f'x{idx}'] = x\n",
        "    output += weights[idx]*x\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(len(weights),))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')\n",
        "\n",
        "  linear_model.fit(np.column_stack([inputs[f'x{idx}'] for idx in range(len(weights))]),output,epochs=100,batch_size=10, verbose=0)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weights = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weights= [0.5,0.8,-0.4,-0.2] #<--- input values here\n",
        "bias = 0.3 #<--- input value here\n",
        "\n",
        "linear_fit(weights, bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPkovihiCnZx",
        "outputId": "64f13f9e-67b0-4599-a771-e561682949d8"
      },
      "id": "yPkovihiCnZx",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear model weights = [[ 0.50016195]\n",
            " [ 0.8001223 ]\n",
            " [-0.39946705]\n",
            " [-0.19962049]]\n",
            "linear model bias = [0.30025494]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks good so far, but what if we want to start building up our network with more neurons to fit more complex functions?\n",
        "\n",
        "Let's go back to our model from the start, linking a single input $x$ to a single output $y$, but this time pass $x$ through three neurons, $n_1, n_2,$ and $n_3$, instead of just one.\n",
        "\n",
        "Given we know now what each neuron is doing, it's easy to construct a function linking $x$ to $y$:\n",
        "\n",
        "\\begin{equation}\n",
        "y = f(w_3f(w_2f(w_1x+b_1)+b_2)+b_3).\n",
        "\\end{equation}\n",
        "\n",
        "Now we know what is happening within each neuron, let's move on to the activation function."
      ],
      "metadata": {
        "id": "xxFRTwARS-iq"
      },
      "id": "xxFRTwARS-iq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Activation functions\n",
        "Activation functions are used to introduce non-linearities into neural networks to allow us to fit more complex functions.\n",
        "\n",
        "Before, we just used a linear activation function of the form\n",
        "\\begin{equation}\n",
        "f(\\textbf{w}\\cdot\\textbf{x} + b) = \\alpha(\\textbf{w}\\cdot\\textbf{x} + b) + \\beta,\n",
        "\\end{equation}\n",
        "where $(\\alpha, \\beta) = (1,0)$.\n",
        "\n",
        "It turns out linear activation functions aren't a good choice if we're looking for non-linearity!\n",
        "\n",
        "If we take our equation for a series of neurons from before:\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\alpha w_3(\\alpha w_2(\\alpha w_1x + b_3 + \\beta) + b_ 2 + \\beta) + b_3 + \\beta,\n",
        "\\end{equation}\n",
        "\n",
        "Given that our neuron weights and biases, $w_n$ and $b_n$, and our linear activation function terms, $\\alpha$ and $\\beta$, are all constants, this equation for our fit collapses down to\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\alpha^3 w_3 w_2 w_1 x + \\gamma,\n",
        "\\end{equation}\n",
        "\n",
        "where $\\gamma$ is a constant. It doesn't matter how much we tune our $w_n$ terms here, we're stuck with a linear fit!\n",
        "\n",
        "Due to its computational efficiency, popular choice is the Rectified Linear Unit (ReLU) function:\n",
        "\\begin{equation}\n",
        "f(x) = \\textrm{max}(0,x),\n",
        "\\end{equation}\n",
        "Plotting this, we get:"
      ],
      "metadata": {
        "id": "gtywYgT1SCfF"
      },
      "id": "gtywYgT1SCfF"
    },
    {
      "cell_type": "code",
      "source": [
        "def rectified_linear_unit(x):\n",
        "  return max(0.0, x)\n",
        "\n",
        "x = np.linspace(-1,1,100)\n",
        "y = [rectified_linear_unit(x_val) for x_val in x]\n",
        "plt.plot(x,y)\n",
        "plt.axvline(0, color='black')"
      ],
      "metadata": {
        "id": "pdtsK_llg7tM",
        "outputId": "6d483a78-36db-4c14-b9b5-70b4155fd56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "id": "pdtsK_llg7tM",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7e448c497850>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be thinking that this still seems very close to a linear function.\n",
        "\n",
        "However, ReLU doesn't stick to the condition of a linear function that requires\n",
        "\\begin{equation}\n",
        "f(x) + f(y) = f(x+y),\n",
        "\\end{equation}\n",
        "because for ReLU $f(-1) + f(1) \\neq f(0)$.\n",
        "\n",
        "This means we avoid the function for our series of neurons collapsing down to just a linear fit, and we have non-linearity!\n",
        "\n",
        "Now, by increasing the number of layers in our network, and the number of neurons per layer, we're able to effectively reproduce complex functions present in a dataset used for training.\n",
        "\n",
        "For the rest of this notebook we'll stick to just using ReLU, but I'll show you a couple of other activation functions here:"
      ],
      "metadata": {
        "id": "t9wbCrPQjUg7"
      },
      "id": "t9wbCrPQjUg7"
    },
    {
      "cell_type": "code",
      "source": [
        "### ACTIVATION FUNCTIONS ###"
      ],
      "metadata": {
        "id": "sMjdy8Evm3VO"
      },
      "id": "sMjdy8Evm3VO",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3) Loss functions\n",
        "The last part of a basic neural network we need to learn about is the ***loss function*** - this is how our neural network determines how well it is fitting the data.\n",
        "\n",
        "For a every point in our dataset of $N$ points, each with an $x$ and $y$ value, we show our network the input $x$ and it makes a prediction $\\hat{y}$.\n",
        "\n",
        "A common choice is to take the Mean Squared Error (MSE), averaged over all $N$ points:\n",
        "\\begin{equation}\n",
        "\\textrm{MSE} = \\frac{1}{N} \\sum_{y=i}^{N} (y_i-\\hat{y}_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "During training, our network tunes the neuron weights and biases in a way that decreases the MSE over the dataset.\n",
        "\n",
        "I turned off the in-training network printout from the cells for our single neuron, but lets run them again with the printout enabled (setting verbose=1 in model.fit) to see what we get:"
      ],
      "metadata": {
        "id": "GXtJj-ItoG-U"
      },
      "id": "GXtJj-ItoG-U"
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_fit(weight, bias):\n",
        "  ##### generate inputs #####\n",
        "  x = np.random.rand(1000)\n",
        "\n",
        "  ##### generate outputs #####\n",
        "  y = weight*x + bias\n",
        "\n",
        "  ##### create tensorflow model #####\n",
        "  tf.keras.backend.clear_session()\n",
        "  linear_model = tf.keras.Sequential()#instantiate model\n",
        "\n",
        "  linear_model.add(tf.keras.Input(1,))#input layer\n",
        "\n",
        "  linear_model.add(tf.keras.layers.Dense(units=1, activation=\"linear\",))#single dense layer, single neuron, linear activation\n",
        "\n",
        "  linear_model.compile(loss='mse')\n",
        "\n",
        "  linear_model.fit(x,y,epochs=50,batch_size=10, verbose=1)\n",
        "\n",
        "  ##### print neuron weights and biases #####\n",
        "  print(\"linear model weight = \" + str(linear_model.layers[0].get_weights()[0]))\n",
        "  print(\"linear model bias = \" + str(linear_model.layers[0].get_weights()[1]))\n",
        "\n",
        "weight= 0.5 #<--- input value here\n",
        "bias = 0.8 #<--- input value here\n",
        "\n",
        "linear_fit(weight, bias)"
      ],
      "metadata": {
        "id": "zrHf-bNvqa9e",
        "outputId": "6023f5fb-f488-4582-de35-846316ae10f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zrHf-bNvqa9e",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 1s 2ms/step - loss: 2.2699\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.8381\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.4577\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.1248\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.8391\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.5884\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.3893\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.2328\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.1204\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0508\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0203\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0134\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0088\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0053\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0026\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 9.6422e-04\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.5174e-04\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 8.7944e-07\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.4125e-07\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 4.9615e-07\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.4103e-07\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.3062e-07\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.0926e-07\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.2295e-07\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.3355e-07\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 4.8935e-07\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.1247e-07\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.3546e-07\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.2505e-07\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.3473e-07\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.0476e-07\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 5.1383e-07\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.4396e-07\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.2959e-07\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.1961e-07\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.0531e-07\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.4153e-07\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.1195e-07\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.3212e-07\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 4.9981e-07\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.3424e-07\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.2093e-07\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.1188e-07\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.3469e-07\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 4.9520e-07\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.2374e-07\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.3308e-07\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.2945e-07\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.4130e-07\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 5.0569e-07\n",
            "linear model weight = [[0.50040406]]\n",
            "linear model bias = [0.8000805]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, you should get a series of readouts that look something like this:\n",
        "\n",
        "> Epoch 1/50\\\n",
        "100/100 [==============================] - 1s 2ms/step - loss: 2.2699\\\n",
        "Epoch 2/50\\\n",
        "100/100 [==============================] - 0s 2ms/step - loss: 1.8381\\\n",
        "Epoch 3/50\\\n",
        "100/100 [==============================] - 0s 1ms/step - loss: 1.4577,\n",
        "\n",
        "with the 'loss' value decreasing each epoch (each pass through the dataset). This is your neural network learning!"
      ],
      "metadata": {
        "id": "b-ZwWU5AqlKn"
      },
      "id": "b-ZwWU5AqlKn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Neural Network Caveats\n",
        "Now we know what's going on inside a neural network, let's go over the limitations and the approach we need to take for preparing data for training.\n"
      ],
      "metadata": {
        "id": "Cb8n_Y7RnJaC"
      },
      "id": "Cb8n_Y7RnJaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Overfitting\n",
        "One of the main problems in training neural networks is the risk of our network overfitting on data we used to train our network.\n",
        "\n",
        "The best way to explain this is graphically. Let's say we have a"
      ],
      "metadata": {
        "id": "ceSDX2e5z150"
      },
      "id": "ceSDX2e5z150"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}